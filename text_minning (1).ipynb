{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "123-456-7890 is a valid phone number.\n",
      "example@example.com is a valid email address.\n",
      "123-45-6789 is a valid social security number.\n",
      "Phone numbers found: ['123-456-7890']\n",
      "Email addresses found: ['example@example.com']\n",
      "Social security number found: 123-45-6789\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def validate_phone_number(phone_number):\n",
    "    # Regular expression pattern for phone number validation\n",
    "    pattern = r'^\\d{3}-\\d{3}-\\d{4}$'\n",
    "    if re.match(pattern, phone_number):\n",
    "        print(f'{phone_number} is a valid phone number.')\n",
    "    else:\n",
    "        print(f'{phone_number} is not a valid phone number.')\n",
    "\n",
    "def validate_email(email):\n",
    "    # Regular expression pattern for email validation\n",
    "    pattern = r'^[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}$'\n",
    "    if re.match(pattern, email):\n",
    "        print(f'{email} is a valid email address.')\n",
    "    else:\n",
    "        print(f'{email} is not a valid email address.')\n",
    "\n",
    "def validate_social_security_number(ssn):\n",
    "    # Regular expression pattern for social security number validation\n",
    "    pattern = r'^\\d{3}-\\d{2}-\\d{4}$'\n",
    "    if re.match(pattern, ssn):\n",
    "        print(f'{ssn} is a valid social security number.')\n",
    "    else:\n",
    "        print(f'{ssn} is not a valid social security number.')\n",
    "\n",
    "def extract_phone_numbers(text):\n",
    "    # Regular expression pattern to extract phone numbers from text\n",
    "    pattern = r'\\d{3}-\\d{3}-\\d{4}'\n",
    "    phone_numbers = re.findall(pattern, text)\n",
    "    print('Phone numbers found:', phone_numbers)\n",
    "\n",
    "def extract_emails(text):\n",
    "    # Regular expression pattern to extract email addresses from text\n",
    "    pattern = r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b'\n",
    "    emails = re.findall(pattern, text)\n",
    "    print('Email addresses found:', emails)\n",
    "\n",
    "def extract_social_security_numbers(text):\n",
    "    # Regular expression pattern to extract social security numbers from text\n",
    "    pattern = r'\\d{3}-\\d{2}-\\d{4}'\n",
    "    ssn = re.search(pattern, text)\n",
    "    if ssn:\n",
    "        print('Social security number found:', ssn.group())\n",
    "    else:\n",
    "        print('No social security number found.')\n",
    "\n",
    "# Phone number validation\n",
    "phone_number = '123-456-7890'\n",
    "validate_phone_number(phone_number)\n",
    "\n",
    "# Email address validation\n",
    "email = 'example@example.com'\n",
    "validate_email(email)\n",
    "\n",
    "# Social security number validation\n",
    "ssn = '123-45-6789'\n",
    "validate_social_security_number(ssn)\n",
    "\n",
    "# Text extraction examples\n",
    "text = 'Contact me at 123-456-7890 or email me at example@example.com. My SSN is 123-45-6789.'\n",
    "extract_phone_numbers(text)\n",
    "extract_emails(text)\n",
    "extract_social_security_numbers(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.google.com/imghp?hl=en&tab=wi\n",
      "https://maps.google.co.in/maps?hl=en&tab=wl\n",
      "https://play.google.com/?hl=en&tab=w8\n",
      "https://www.youtube.com/?tab=w1\n",
      "https://news.google.com/?tab=wn\n",
      "https://mail.google.com/mail/?tab=wm\n",
      "https://drive.google.com/?tab=wo\n",
      "https://www.google.co.in/intl/en/about/products?tab=wh\n",
      "http://www.google.co.in/history/optout?hl=en\n",
      "/preferences?hl=en\n",
      "https://accounts.google.com/ServiceLogin?hl=en&passive=true&continue=https://www.google.com/&ec=GAZAAQ\n",
      "/advanced_search?hl=en-IN&authuser=0\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=hi&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCAU\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=bn&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCAY\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=te&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCAc\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=mr&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCAg\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=ta&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCAk\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=gu&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCAo\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=kn&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCAs\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=ml&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCAw\n",
      "https://www.google.com/setprefs?sig=0_EbHV7eKkmtwPws-8JCAS4XP6wvI%3D&hl=pa&source=homepage&sa=X&ved=0ahUKEwiA0qe22Mb_AhVngFYBHSEIALgQ2ZgBCA0\n",
      "/intl/en/ads/\n",
      "http://www.google.co.in/services/\n",
      "/intl/en/about.html\n",
      "https://www.google.com/setprefdomain?prefdom=IN&prev=https://www.google.co.in/&sig=K_0LsEniLFZYnymb_tFhxoXc-QxJA%3D\n",
      "/intl/en/policies/privacy/\n",
      "/intl/en/policies/terms/\n"
     ]
    }
   ],
   "source": [
    "#parsing html doc\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def find_hyperlinks(url):\n",
    "    # Send a GET request to the URL and retrieve the HTML content\n",
    "    response = requests.get(url)\n",
    "    html_content = response.text\n",
    "\n",
    "    # Parse the HTML content using BeautifulSoup\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "\n",
    "    # Find all anchor tags (hyperlinks) in the parsed HTML\n",
    "    anchor_tags = soup.find_all('a')\n",
    "\n",
    "    # Extract the href attribute (URL) from each anchor tag\n",
    "    hyperlinks = [tag['href'] for tag in anchor_tags if 'href' in tag.attrs]\n",
    "\n",
    "    # Print the found hyperlinks\n",
    "    for hyperlink in hyperlinks:\n",
    "        print(hyperlink)\n",
    "\n",
    "# Provide the URL of the website you want to parse\n",
    "website_url = 'https://www.google.com'\n",
    "\n",
    "# Call the function to find and print all hyperlinks on the website\n",
    "find_hyperlinks(website_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: excellent recommended highly service wa\n",
      "Topic 1: this the it worth did\n"
     ]
    }
   ],
   "source": [
    "#discover abstract topics\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "\n",
    "# Sample collection of documents\n",
    "documents = [\n",
    "    \"I love this movie, it's so entertaining\",\n",
    "    \"The food at this restaurant is delicious\",\n",
    "    \"The service was excellent, highly recommended\",\n",
    "    \"I didn't enjoy this product, it's not worth the price\",\n",
    "]\n",
    "\n",
    "# Preprocess the documents\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "preprocessed_docs = []\n",
    "for doc in documents:\n",
    "    # Tokenize the document and lemmatize the words\n",
    "    tokens = [lemmatizer.lemmatize(word.lower()) for word in word_tokenize(doc)]\n",
    "    preprocessed_docs.append(' '.join(tokens))\n",
    "\n",
    "# Create a bag-of-words representation of the documents\n",
    "vectorizer = CountVectorizer()\n",
    "feature_matrix = vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# Train the LDA model\n",
    "lda_model = LatentDirichletAllocation(n_components=2, random_state=42)\n",
    "lda_model.fit(feature_matrix)\n",
    "\n",
    "# Print the discovered topics\n",
    "feature_names = vectorizer.get_feature_names()\n",
    "topics = lda_model.components_\n",
    "for topic_idx, topic in enumerate(topics):\n",
    "    top_words = [feature_names[word_idx] for word_idx in topic.argsort()[:-6:-1]]\n",
    "    print(f\"Topic {topic_idx}: {' '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted category: positive\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#classification of document\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Sample documents and their corresponding categories\n",
    "documents = [\n",
    "    (\"I love this movie\", \"positive\"),\n",
    "    (\"This restaurant has great food\", \"positive\"),\n",
    "    (\"The service was terrible\", \"negative\"),\n",
    "    (\"I don't like this product\", \"negative\"),\n",
    "]\n",
    "\n",
    "# Preprocess the documents\n",
    "nltk.download('punkt')\n",
    "preprocessed_docs = []\n",
    "categories = []\n",
    "for doc, category in documents:\n",
    "    # Tokenize the document\n",
    "    tokens = [word.lower() for word in word_tokenize(doc)]\n",
    "    preprocessed_docs.append(\" \".join(tokens))\n",
    "    categories.append(category)\n",
    "\n",
    "# Create the feature vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "feature_vectors = vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# Train the classifier (Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(feature_vectors, categories)\n",
    "\n",
    "# Classify a new document\n",
    "new_doc = \"This movie is amazing!\"\n",
    "new_preprocessed_doc = \" \".join([word.lower() for word in word_tokenize(new_doc)])\n",
    "new_feature_vector = vectorizer.transform([new_preprocessed_doc])\n",
    "predicted_category = classifier.predict(new_feature_vector)[0]\n",
    "\n",
    "# Print the predicted category for the new document\n",
    "print(f\"Predicted category: {predicted_category}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finding similarity bet document\n",
    "\n",
    "import math\n",
    "import string\n",
    "import sys\n",
    "\n",
    "# reading the text file\n",
    "# This functio will return a\n",
    "# list of the lines of text\n",
    "# in the file.\n",
    "def read_file(filename):\n",
    "\t\n",
    "\ttry:\n",
    "\t\twith open(filename, 'r') as f:\n",
    "\t\t\tdata = f.read()\n",
    "\t\treturn data\n",
    "\t\n",
    "\texcept IOError:\n",
    "\t\tprint(\"Error opening or reading input file: \", filename)\n",
    "\t\tsys.exit()\n",
    "\n",
    "# splitting the text lines into words\n",
    "# translation table is a global variable\n",
    "# mapping upper case to lower case and\n",
    "# punctuation to spaces\n",
    "translation_table = str.maketrans(string.punctuation+string.ascii_uppercase,\n",
    "\t\t\t\t\t\t\t\t\t\" \"*len(string.punctuation)+string.ascii_lowercase)\n",
    "\t\n",
    "# returns a list of the words\n",
    "# in the file\n",
    "def get_words_from_line_list(text):\n",
    "\t\n",
    "\ttext = text.translate(translation_table)\n",
    "\tword_list = text.split()\n",
    "\t\n",
    "\treturn word_list\n",
    "\n",
    "\n",
    "# counts frequency of each word\n",
    "# returns a dictionary which maps\n",
    "# the words to their frequency.\n",
    "def count_frequency(word_list):\n",
    "\t\n",
    "\tD = {}\n",
    "\t\n",
    "\tfor new_word in word_list:\n",
    "\t\t\n",
    "\t\tif new_word in D:\n",
    "\t\t\tD[new_word] = D[new_word] + 1\n",
    "\t\t\t\n",
    "\t\telse:\n",
    "\t\t\tD[new_word] = 1\n",
    "\t\t\t\n",
    "\treturn D\n",
    "\n",
    "# returns dictionary of (word, frequency)\n",
    "# pairs from the previous dictionary.\n",
    "def word_frequencies_for_file(filename):\n",
    "\t\n",
    "\tline_list = read_file(filename)\n",
    "\tword_list = get_words_from_line_list(line_list)\n",
    "\tfreq_mapping = count_frequency(word_list)\n",
    "\n",
    "\tprint(\"File\", filename, \":\", )\n",
    "\tprint(len(line_list), \"lines, \", )\n",
    "\tprint(len(word_list), \"words, \", )\n",
    "\tprint(len(freq_mapping), \"distinct words\")\n",
    "\n",
    "\treturn freq_mapping\n",
    "\n",
    "\n",
    "# returns the dot product of two documents\n",
    "def dotProduct(D1, D2):\n",
    "\tSum = 0.0\n",
    "\t\n",
    "\tfor key in D1:\n",
    "\t\t\n",
    "\t\tif key in D2:\n",
    "\t\t\tSum += (D1[key] * D2[key])\n",
    "\t\t\t\n",
    "\treturn Sum\n",
    "\n",
    "# returns the angle in radians\n",
    "# between document vectors\n",
    "def vector_angle(D1, D2):\n",
    "\tnumerator = dotProduct(D1, D2)\n",
    "\tdenominator = math.sqrt(dotProduct(D1, D1)*dotProduct(D2, D2))\n",
    "\t\n",
    "\treturn math.acos(numerator / denominator)\n",
    "\n",
    "\n",
    "def documentSimilarity(filename_1, filename_2):\n",
    "\t\n",
    "# filename_1 = sys.argv[1]\n",
    "# filename_2 = sys.argv[2]\n",
    "\tsorted_word_list_1 = word_frequencies_for_file(filename_1)\n",
    "\tsorted_word_list_2 = word_frequencies_for_file(filename_2)\n",
    "\tdistance = vector_angle(sorted_word_list_1, sorted_word_list_2)\n",
    "\t\n",
    "\tprint(\"The distance between the documents is: % 0.6f (radians)\"% distance)\n",
    "\t\n",
    "# Driver code\n",
    "documentSimilarity('GFG.txt', 'file.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted label: spam\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#message is spam or not\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "# Sample training data (messages labeled as spam or not spam)\n",
    "train_data = [\n",
    "    (\"Free money!!!\", \"spam\"),\n",
    "    (\"Hello, how are you?\", \"not spam\"),\n",
    "    (\"Get a discount on your next purchase\", \"spam\"),\n",
    "    (\"Meeting at 6 PM\", \"not spam\"),\n",
    "]\n",
    "\n",
    "# Preprocess the training data\n",
    "nltk.download('punkt')\n",
    "preprocessed_messages = []\n",
    "labels = []\n",
    "for message, label in train_data:\n",
    "    # Tokenize the message\n",
    "    tokens = [word.lower() for word in word_tokenize(message)]\n",
    "    preprocessed_messages.append(\" \".join(tokens))\n",
    "    labels.append(label)\n",
    "\n",
    "# Create the feature vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "feature_vectors = vectorizer.fit_transform(preprocessed_messages)\n",
    "\n",
    "# Train the classifier (Naive Bayes)\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(feature_vectors, labels)\n",
    "\n",
    "# Classify a new message\n",
    "new_message = \"Free money!!!\"\n",
    "new_preprocessed_message = \" \".join([word.lower() for word in word_tokenize(new_message)])\n",
    "new_feature_vector = vectorizer.transform([new_preprocessed_message])\n",
    "predicted_label = classifier.predict(new_feature_vector)[0]\n",
    "\n",
    "# Print the predicted label for the new message\n",
    "print(f\"Predicted label: {predicted_label}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text:\n",
      "\n",
      "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the development of algorithms and models to enable computers to understand, interpret, and generate human language. NLP has various applications, including sentiment analysis, text classification, machine translation, and question answering, among others.\n",
      "\n",
      "\n",
      "Tokenization:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focuses', 'on', 'the', 'interaction', 'between', 'computers', 'and', 'humans', 'through', 'natural', 'language', '.', 'It', 'involves', 'the', 'development', 'of', 'algorithms', 'and', 'models', 'to', 'enable', 'computers', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'NLP', 'has', 'various', 'applications', ',', 'including', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'machine', 'translation', ',', 'and', 'question', 'answering', ',', 'among', 'others', '.']\n",
      "\n",
      "Sentence Tokenization:\n",
      "['\\nNatural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.', 'It involves the development of algorithms and models to enable computers to understand, interpret, and generate human language.', 'NLP has various applications, including sentiment analysis, text classification, machine translation, and question answering, among others.']\n",
      "\n",
      "Lemmatization:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'is', 'a', 'subfield', 'of', 'artificial', 'intelligence', 'that', 'focus', 'on', 'the', 'interaction', 'between', 'computer', 'and', 'human', 'through', 'natural', 'language', '.', 'It', 'involves', 'the', 'development', 'of', 'algorithm', 'and', 'model', 'to', 'enable', 'computer', 'to', 'understand', ',', 'interpret', ',', 'and', 'generate', 'human', 'language', '.', 'NLP', 'ha', 'various', 'application', ',', 'including', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'machine', 'translation', ',', 'and', 'question', 'answering', ',', 'among', 'others', '.']\n",
      "\n",
      "Stemming:\n",
      "['natur', 'languag', 'process', '(', 'nlp', ')', 'is', 'a', 'subfield', 'of', 'artifici', 'intellig', 'that', 'focus', 'on', 'the', 'interact', 'between', 'comput', 'and', 'human', 'through', 'natur', 'languag', '.', 'It', 'involv', 'the', 'develop', 'of', 'algorithm', 'and', 'model', 'to', 'enabl', 'comput', 'to', 'understand', ',', 'interpret', ',', 'and', 'gener', 'human', 'languag', '.', 'nlp', 'ha', 'variou', 'applic', ',', 'includ', 'sentiment', 'analysi', ',', 'text', 'classif', ',', 'machin', 'translat', ',', 'and', 'question', 'answer', ',', 'among', 'other', '.']\n",
      "\n",
      "Stopword Removal:\n",
      "['Natural', 'Language', 'Processing', '(', 'NLP', ')', 'subfield', 'artificial', 'intelligence', 'focuses', 'interaction', 'computers', 'humans', 'natural', 'language', '.', 'involves', 'development', 'algorithms', 'models', 'enable', 'computers', 'understand', ',', 'interpret', ',', 'generate', 'human', 'language', '.', 'NLP', 'various', 'applications', ',', 'including', 'sentiment', 'analysis', ',', 'text', 'classification', ',', 'machine', 'translation', ',', 'question', 'answering', ',', 'among', 'others', '.']\n",
      "\n",
      "Frequency Distribution:\n",
      "[(',', 7), ('and', 4), ('.', 3), ('NLP', 2), ('of', 2)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Python program to demonstrate Basic NLP \n",
    "#Tasks with NLTK.\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "\n",
    "# Sample text\n",
    "text = \"\"\"\n",
    "Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language. It involves the development of algorithms and models to enable computers to understand, interpret, and generate human language. NLP has various applications, including sentiment analysis, text classification, machine translation, and question answering, among others.\n",
    "\"\"\"\n",
    "\n",
    "# Tokenization\n",
    "tokens = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "\n",
    "# Lemmatization\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "# Stemming\n",
    "stemmer = PorterStemmer()\n",
    "stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Stopword removal\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "\n",
    "# Frequency distribution\n",
    "fdist = FreqDist(tokens)\n",
    "\n",
    "# Print the results\n",
    "print(\"Original text:\")\n",
    "print(text)\n",
    "print()\n",
    "\n",
    "print(\"Tokenization:\")\n",
    "print(tokens)\n",
    "print()\n",
    "\n",
    "print(\"Sentence Tokenization:\")\n",
    "print(sentences)\n",
    "print()\n",
    "\n",
    "print(\"Lemmatization:\")\n",
    "print(lemmatized_tokens)\n",
    "print()\n",
    "\n",
    "print(\"Stemming:\")\n",
    "print(stemmed_tokens)\n",
    "print()\n",
    "\n",
    "print(\"Stopword Removal:\")\n",
    "print(filtered_tokens)\n",
    "print()\n",
    "\n",
    "print(\"Frequency Distribution:\")\n",
    "print(fdist.most_common(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster 0:\n",
      "- I love this movie, it's so entertaining\n",
      "- The food at this restaurant is delicious\n",
      "- I didn't enjoy this product, it's not worth the price\n",
      "\n",
      "Cluster 1:\n",
      "- The service was excellent, highly recommended\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#Write a Python program to cluster documents using \n",
    "#clustering algorithms\n",
    "\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Read documents from a .txt file\n",
    "\n",
    "# file_path = \"documents.txt\"\n",
    "\n",
    "# with open(file_path, \"r\") as file:\n",
    "#     documents = file.readlines()\n",
    "documents = [\n",
    "    \"I love this movie, it's so entertaining\",\n",
    "    \"The food at this restaurant is delicious\",\n",
    "    \"The service was excellent, highly recommended\",\n",
    "    \"I didn't enjoy this product, it's not worth the price\",\n",
    "]\n",
    "# Preprocess the documents\n",
    "nltk.download('punkt')\n",
    "preprocessed_docs = []\n",
    "for doc in documents:\n",
    "    # Tokenize the document\n",
    "    tokens = nltk.word_tokenize(doc)\n",
    "    preprocessed_docs.append(\" \".join(tokens))\n",
    "\n",
    "# Create the feature vectors using TF-IDF\n",
    "vectorizer = TfidfVectorizer()\n",
    "feature_vectors = vectorizer.fit_transform(preprocessed_docs)\n",
    "\n",
    "# Cluster the documents using K-means\n",
    "num_clusters = 2  # Change this value to set the number of clusters\n",
    "kmeans = KMeans(n_clusters=num_clusters)\n",
    "kmeans.fit(feature_vectors)\n",
    "\n",
    "# Get the cluster labels for the documents\n",
    "cluster_labels = kmeans.labels_\n",
    "\n",
    "# Print the clusters and the documents in each cluster\n",
    "clusters = {}\n",
    "for doc, cluster_label in zip(documents, cluster_labels):\n",
    "    if cluster_label not in clusters:\n",
    "        clusters[cluster_label] = []\n",
    "    clusters[cluster_label].append(doc)\n",
    "\n",
    "for cluster_label, cluster_docs in clusters.items():\n",
    "    print(f\"Cluster {cluster_label}:\")\n",
    "    for doc in cluster_docs:\n",
    "        print(f\"- {doc}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Tejas Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data] Downloading package maxent_ne_chunker to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data] Downloading package words to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\words.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Named entities in the sentence:\n",
      "Apple - PERSON\n",
      "New York City - GPE\n",
      "\n",
      "Named entities in the sentence:\n",
      "John - PERSON\n",
      "Smith CEO - PERSON\n",
      "\n",
      "Named entities in the sentence:\n",
      "United States - GPE\n",
      "North America - GPE\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#a Python program to identify Named Entities \n",
    "#(NE) from document corpus. \n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "\n",
    "# Sample corpus of documents\n",
    "documents = [\n",
    "    \"Apple Inc. is planning to open a new store in New York City.\",\n",
    "    \"John Smith is the CEO of XYZ Corporation.\",\n",
    "    \"The United States is located in North America.\"\n",
    "]\n",
    "\n",
    "# Download the required NLTK resources\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "# Preprocess the documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "for doc_text in documents:\n",
    "    # Tokenize the document into sentences\n",
    "    sentences = sent_tokenize(doc_text)\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        # Tokenize the sentence into words\n",
    "        tokens = word_tokenize(sentence)\n",
    "        \n",
    "        # Filter out stopwords\n",
    "        filtered_tokens = [token for token in tokens if token.lower() not in stop_words]\n",
    "        \n",
    "        # Perform part-of-speech (POS) tagging\n",
    "        tagged_tokens = pos_tag(filtered_tokens)\n",
    "        \n",
    "        # Perform named entity recognition (NER)\n",
    "        named_entities = ne_chunk(tagged_tokens)\n",
    "        \n",
    "        # Print the named entities\n",
    "        print(\"Named entities in the sentence:\")\n",
    "        for entity in named_entities:\n",
    "            if hasattr(entity, 'label'):\n",
    "                print(' '.join(e[0] for e in entity.leaves()), \"-\", entity.label())\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spelling recommendations for 'speling':\n",
      "['speckling', 'spring', 'swelling', 'staling', 'spiking', 'shelling', 'sparing', 'spelling', 'sidling', 'styling', 'seeming', 'smiling', 'spline', 'spaning', 'spalding', 'upfling', 'sealing', 'sporeling', 'spiling', 'keeling', 'scaling', 'sibling', 'seeking', 'seeing', 'specking', 'puling', 'adeling', 'sideling', 'peeling', 'sperling', 'suppling', 'speldring', 'spelunk', 'spading', 'skeeling', 'stealing', 'sparling', 'speaking', 'steeling', 'heling', 'shieling', 'spalling', 'spicing', 'splint', 'skelping', 'smelling', 'shelving', 'sewing', 'spending', 'feeling', 'stelling', 'spacing', 'siering', 'sipling', 'spewing', 'dueling', 'speeding', 'spoiling', 'selling', 'paling', 'spelding', 'speering', 'spurling', 'petling', 'simling', 'pewing', 'sling', 'spirling', 'spearing', 'sailing', 'pelting', 'piling', 'slewing', 'sterling', 'sapling', 'apeling', 'spaying', 'spatling', 'shealing', 'seedling', 'soiling', 'opening', 'swaling', 'spiring']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# Download the NLTK corpus of words (if not already downloaded)\n",
    "nltk.download('words')\n",
    "\n",
    "# Load the English words corpus\n",
    "english_words = set(nltk.corpus.words.words())\n",
    "\n",
    "def get_spelling_recommendations(misspelled_word):\n",
    "    # Convert the misspelled word to lowercase\n",
    "    misspelled_word = misspelled_word.lower()\n",
    "    \n",
    "    # Find words similar to the misspelling\n",
    "    recommendations = []\n",
    "    for word in english_words:\n",
    "        # Calculate the Levenshtein distance between the misspelled word and each word in the English words corpus\n",
    "        distance = nltk.edit_distance(misspelled_word, word)\n",
    "        \n",
    "        # Add words with a low edit distance as recommendations\n",
    "        if distance <= 2:\n",
    "            recommendations.append(word)\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Example usage\n",
    "misspelled = 'speling'\n",
    "recommendations = get_spelling_recommendations(misspelled)\n",
    "print(f\"Spelling recommendations for '{misspelled}':\")\n",
    "print(recommendations)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package reuters to C:\\Users\\Tejas\n",
      "[nltk_data]     Pundpal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package reuters is already up-to-date!\n",
      "<ipython-input-37-f8bc136bda2a>:45: RuntimeWarning: overflow encountered in add\n",
      "  word_vectors[target_index] += learning_rate * word_vectors[context_index]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import reuters\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "\n",
    "# Download the Reuters corpus (if not already downloaded)\n",
    "nltk.download('reuters')\n",
    "\n",
    "# Load the corpus\n",
    "corpus = reuters.sents()\n",
    "\n",
    "# Tokenize the corpus and build vocabulary\n",
    "vocabulary = defaultdict(int)\n",
    "for document in corpus:\n",
    "    tokens = word_tokenize(' '.join(document))\n",
    "    for token in tokens:\n",
    "        vocabulary[token.lower()] += 1\n",
    "\n",
    "# Filter vocabulary to include only frequently occurring words\n",
    "min_word_count = 5\n",
    "filtered_vocab = [word for word, count in vocabulary.items() if count >= min_word_count]\n",
    "\n",
    "# Assign indices to words in the vocabulary\n",
    "word2index = {word: index for index, word in enumerate(filtered_vocab)}\n",
    "\n",
    "# Initialize word vectors with random values\n",
    "embedding_dim = 100\n",
    "word_vectors = np.random.uniform(low=-0.5, high=0.5, size=(len(filtered_vocab), embedding_dim))\n",
    "\n",
    "# Train the Word2Vec model by updating word vectors\n",
    "window_size = 5\n",
    "learning_rate = 0.025\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for document in corpus:\n",
    "        tokens = [token.lower() for token in word_tokenize(' '.join(document))]\n",
    "        for i, target_word in enumerate(tokens):\n",
    "            for j in range(max(0, i - window_size), min(i + window_size + 1, len(tokens))):\n",
    "                context_word = tokens[j]\n",
    "                if target_word in filtered_vocab and context_word in filtered_vocab:\n",
    "                    target_index = word2index[target_word]\n",
    "                    context_index = word2index[context_word]\n",
    "                    word_vectors[target_index] += learning_rate * word_vectors[context_index]\n",
    "\n",
    "# Example usage: Get word embeddings\n",
    "word_embeddings = {word: word_vectors[index] for word, index in word2index.items()}\n",
    "\n",
    "# Print the most similar words to a given word\n",
    "word = \"money\"\n",
    "similar_words = sorted(word_embeddings.keys(), key=lambda w: np.dot(word_embeddings[word], word_embeddings[w]), reverse=True)\n",
    "print(f\"Most similar words to '{word}':\")\n",
    "print(similar_words[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "def identify_sentiment(text):\n",
    "    # Create a TextBlob object with the given text\n",
    "    blob = TextBlob(text)\n",
    "    \n",
    "    # Get the sentiment polarity (-1 to 1) and subjectivity (0 to 1)\n",
    "    polarity = blob.sentiment.polarity\n",
    "    subjectivity = blob.sentiment.subjectivity\n",
    "    \n",
    "    # Determine the sentiment label based on polarity\n",
    "    if polarity > 0:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif polarity < 0:\n",
    "        sentiment_label = 'Negative'\n",
    "    else:\n",
    "        sentiment_label = 'Neutral'\n",
    "    \n",
    "    return sentiment_label, polarity, subjectivity\n",
    "\n",
    "# Example usage\n",
    "text = \"I really enjoyed the movie. It was fantastic!\"\n",
    "sentiment, polarity, subjectivity = identify_sentiment(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Sentiment:\", sentiment)\n",
    "print(\"Polarity:\", polarity)\n",
    "print(\"Subjectivity:\", subjectivity)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_sentiment(text):\n",
    "    # Define lists of positive and negative words\n",
    "    positive_words = ['good', 'happy', 'great', 'joyful']\n",
    "    negative_words = ['bad', 'sad', 'terrible', 'unhappy']\n",
    "\n",
    "    # Tokenize the text into individual words\n",
    "    words = text.lower().split()\n",
    "\n",
    "    # Count the number of positive and negative words\n",
    "    num_positive_words = sum(word in positive_words for word in words)\n",
    "    num_negative_words = sum(word in negative_words for word in words)\n",
    "\n",
    "    # Determine the sentiment label based on word counts\n",
    "    if num_positive_words > num_negative_words:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif num_positive_words < num_negative_words:\n",
    "        sentiment_label = 'Negative'\n",
    "    else:\n",
    "        sentiment_label = 'Neutral'\n",
    "\n",
    "    return sentiment_label\n",
    "\n",
    "# Example usage\n",
    "text = \"I am feeling really happy today!\"\n",
    "sentiment = identify_sentiment(text)\n",
    "\n",
    "print(\"Text:\", text)\n",
    "print(\"Sentiment:\", sentiment)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
